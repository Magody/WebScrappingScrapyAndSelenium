{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# relevant packages & modules\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "import time\n",
    "import json\n",
    "from tqdm.notebook import tqdm\n",
    "import os\n",
    "from IPython.display import clear_output\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "class ItemAnimeFLV:\n",
    "    \n",
    "    attributes = []\n",
    "    \n",
    "    def set_attributes(self, dict_attributes:dict):\n",
    "        self.attributes = []\n",
    "        for key,value in dict_attributes.items():\n",
    "            setattr(self, key, value)\n",
    "            self.attributes.append(key)\n",
    "    \n",
    "    def get_serie(self, map_attributes_to_columns=dict()):\n",
    "        if len(self.attributes) == 0:\n",
    "            print(\"First add atributes with set_attributes\")\n",
    "            return pd.Series()\n",
    "        \n",
    "        if len(map_attributes_to_columns) > 0:\n",
    "            # not supported\n",
    "            return pd.Series()\n",
    "        \n",
    "        data = dict()\n",
    "        for attribute in self.attributes:\n",
    "            data[attribute] = getattr(self, attribute)\n",
    "        \n",
    "        return pd.Series(data=data)\n",
    "            \n",
    "\n",
    "\n",
    "class ScrapperAnimeFLV:\n",
    "    \n",
    "    driver = None\n",
    "    \n",
    "    logged_in = False\n",
    "    restarting = False\n",
    "    \n",
    "    cache = dict()\n",
    "    \n",
    "    stop = False\n",
    "    \n",
    "    def __init__(self, path_driver_chrome, start=True, headless=False, smoth = 800, wait_smoth = 1):\n",
    "        \n",
    "        \n",
    "        self.smoth = smoth\n",
    "        self.wait_smoth = wait_smoth\n",
    "        self.stop = False\n",
    "        \n",
    "        if start:\n",
    "            if headless:\n",
    "                chrome_options = Options()\n",
    "                chrome_options.add_argument('--headless')\n",
    "                self.driver = webdriver.Chrome(executable_path=path_driver_chrome, options=chrome_options)\n",
    "            else:\n",
    "                self.driver = webdriver.Chrome(executable_path=path_driver_chrome)\n",
    "        \n",
    "            self.wait(3)\n",
    "        \n",
    "    def wait(self, t):\n",
    "        time.sleep(t)\n",
    "        \n",
    "    def check_requisites(self):\n",
    "        \n",
    "        if not self.logged_in:\n",
    "            print(\"Please login first\")\n",
    "            return False        \n",
    "        return True\n",
    "    \n",
    "    def login(self, username, password):\n",
    "        \n",
    "        self.driver.find_element(By.CSS_SELECTOR, \"div[class='AFixed']\").click()\n",
    "        self.wait(2)\n",
    "        self.driver.find_element(By.CSS_SELECTOR, \"div[class='Login']\").click()\n",
    "        self.wait(2)\n",
    "        web_element_username = self.driver.find_element(By.CSS_SELECTOR, \"input[name='email']\")\n",
    "        web_element_username.clear()\n",
    "        web_element_username.send_keys(username)\n",
    "\n",
    "        web_element_password = self.driver.find_element(By.CSS_SELECTOR, \"input[name='password']\")\n",
    "        web_element_password.clear()\n",
    "        web_element_password.send_keys(password)\n",
    "        self.driver.find_element(By.CSS_SELECTOR, \"button[type='submit']\").click()\n",
    "        self.wait(3)\n",
    "        self.driver.get(\"https://www3.animeflv.net/browse\")\n",
    "        self.logged_in = True\n",
    "        print(\"Logged in!\")\n",
    "        \n",
    "    def obtain_urls_items(self, url_begin=\"https://www3.animeflv.net/browse?page=1\", early_stop=-1):\n",
    "        \n",
    "        if not self.check_requisites():\n",
    "            return []\n",
    "        \n",
    "        queue = [url_begin]\n",
    "\n",
    "        urls = []\n",
    "\n",
    "        while len(queue) > 0:\n",
    "            \n",
    "            if early_stop != -1 :\n",
    "                if len(urls) > early_stop:\n",
    "                    break\n",
    "                \n",
    "            url_page = queue.pop()\n",
    "\n",
    "            self.driver.get(url_page)\n",
    "            self.wait(1)\n",
    "            \n",
    "            items_cards = self.driver.find_elements(By.XPATH, \"//ul[contains(@class,'ListAnimes')]/li/article/a\")\n",
    "            for card in items_cards:\n",
    "                urls.append(card.get_attribute(\"href\"))\n",
    "\n",
    "            try:\n",
    "                element_next = self.driver.find_element(By.XPATH, \"//ul[@class='pagination']/li[last()]\")\n",
    "                if element_next.get_attribute(\"class\") != \"disabled\":\n",
    "                    url_next = element_next.find_element(By.XPATH, \"./a\").get_attribute(\"href\")\n",
    "                    queue.append(url_next)\n",
    "            except:\n",
    "                print(\"Error at parsing next of\", url_page)\n",
    "\n",
    "        return urls\n",
    "\n",
    "\n",
    "    def scrape_items(self, timers: dict(),url_begin=\"https://www3.animeflv.net/browse?page=1\", urls=None,header=[\"title\",\"title_alternative1\",\"title_alternative2\",\"rating\",\"votes\",\"type_serie\",\"cover\",\"state\",\"followers\",\"categories\",\"related\",\"episodes\",\"description\",\"reactions_like\",\"reactions_funny\",\"reactions_love\",\"reactions_surprise\",\"reactions_angry\",\"reactions_sad\",\"reactions_total\"], early_stop=-1):\n",
    "        \n",
    "        df = pd.DataFrame(columns=header)\n",
    "        \n",
    "        if not self.check_requisites():\n",
    "            return df\n",
    "        \n",
    "        try:\n",
    "            urls_items = urls\n",
    "            if urls is None:\n",
    "                urls_items = self.obtain_urls_items(url_begin=url_begin, early_stop=-1)\n",
    "            \n",
    "            self.cache[\"urls\"] = urls_items\n",
    "            self.cache[\"index_seen\"] = -1\n",
    "            self.cache[\"urls_error\"] = []\n",
    "            self.cache[\"df\"] = df\n",
    "            \n",
    "            \n",
    "            \n",
    "            index_df = 0\n",
    "\n",
    "            timer_load_page = timers.get(\"timer_load_page\", 2)\n",
    "            timer_load_disqus = timers.get(\"timer_load_disqus\",1)\n",
    "            timer_load_reactions = timers.get(\"timer_load_reactions\",1)\n",
    "            \n",
    "            \n",
    "            for url in tqdm(urls_items):\n",
    "                \n",
    "                # initial\n",
    "                self.driver.get(url)\n",
    "                self.wait(timer_load_page)\n",
    "                def extract_first(elements, mapping=None, default=\"none\"):\n",
    "                    elements_len = len(elements)\n",
    "                    if elements_len == 0:\n",
    "                        return default\n",
    "                    \n",
    "                    \n",
    "                    try:\n",
    "                        element = elements[0].text\n",
    "                        if mapping is not None:\n",
    "                            element = mapping(element)\n",
    "                        return element\n",
    "                    except:\n",
    "                        return default\n",
    "\n",
    "\n",
    "                def parse_multiple(web_elements, separator=\"|\"):\n",
    "                    \n",
    "                    out = \"\"\n",
    "                    for i in range(len(web_elements)):\n",
    "                        out += web_elements[i].text\n",
    "                        if i < len(web_elements)-1:\n",
    "                            out += separator        \n",
    "                    return out\n",
    "                    \n",
    "                title = extract_first(self.driver.find_elements(By.CSS_SELECTOR, \"h1.Title\"),default=url)\n",
    "                title_alternatives = self.driver.find_elements(By.CSS_SELECTOR, \"span.TxtAlt\")\n",
    "                title_alternatives_len = len(title_alternatives)\n",
    "                title_alternative1 = \"none\"\n",
    "                title_alternative2 = \"none\"\n",
    "\n",
    "                if title_alternatives_len > 2:\n",
    "                    title_alternative1 = title_alternatives[-2].text\n",
    "                    title_alternative2 = title_alternatives[-1].text\n",
    "                else:\n",
    "                    index_alternative = 0\n",
    "                    if title_alternatives_len > index_alternative:\n",
    "                        title_alternative1 = title_alternatives[index_alternative].text\n",
    "                        index_alternative += 1            \n",
    "                    if title_alternatives_len > index_alternative:\n",
    "                        title_alternative2 = title_alternatives[index_alternative].text\n",
    "                        index_alternative += 1\n",
    "                try:\n",
    "                    rating = extract_first(self.driver.find_elements(By.CSS_SELECTOR, \"span#votes_prmd\"), mapping=float, default=0)\n",
    "                    votes = extract_first(self.driver.find_elements(By.CSS_SELECTOR, \"span#votes_nmbr\"), mapping=int, default=0)\n",
    "                    type_serie = extract_first(self.driver.find_elements(By.CSS_SELECTOR, \"span.Type\"))\n",
    "                    cover = self.driver.find_elements(By.XPATH, \"//div[@class='AnimeCover']/div/figure/img\")\n",
    "                    cover = \"none\" if len(cover) == 0 else cover[0].get_attribute(\"src\")\n",
    "\n",
    "                    state = extract_first(self.driver.find_elements(By.XPATH, '//p[contains(@class,\"AnmStts\")]/span'))\n",
    "                    followers = extract_first(self.driver.find_elements(By.XPATH, '//div[contains(@class,\"Title\")]/span'), mapping=int, default=0)\n",
    "\n",
    "                    categories = parse_multiple(self.driver.find_elements(By.XPATH, '//nav[contains(@class,\"Nvgnrs\")]/a'))\n",
    "\n",
    "                    description = extract_first(self.driver.find_elements(By.XPATH, '//div[contains(@class,\"Description\")]/p'))\n",
    "                    related = parse_multiple(self.driver.find_elements(By.XPATH, '//ul[@class=\"ListAnmRel\"]/li'))\n",
    "\n",
    "                    episodes = int(extract_first(self.driver.find_elements(By.XPATH, '//ul[@id=\"episodeList\"]/li/a/p'), default=\"Episodio 0\").split(\" \")[1])\n",
    "\n",
    "                \n",
    "                except Exception as error1:\n",
    "                    print(\"Error with basic parsing\", url, error1)\n",
    "                    self.cache[\"urls_error\"].append(url)\n",
    "                    self.cache[\"index_seen\"] += 1\n",
    "                    continue\n",
    "                \n",
    "                try:\n",
    "                    # final: frame\n",
    "                    result = self.driver.execute_script('var element = document.querySelector(\"#disqus_thread\"); if(element){element.scrollIntoView();return 1;}else{return 0}')\n",
    "                    \n",
    "                    if result == int(0):\n",
    "                        state = \"404 not found\"\n",
    "                        reactions_like = 0\n",
    "                        reactions_funny = 0\n",
    "                        reactions_love = 0\n",
    "                        reactions_surprise = 0\n",
    "                        reactions_angry = 0\n",
    "                        reactions_sad = 0\n",
    "                    else:\n",
    "                        self.wait(timer_load_disqus)\n",
    "                        frame_disqus = self.driver.find_elements(By.XPATH, '//div[@id=\"disqus_thread\"]/iframe')[0]\n",
    "                        self.driver.switch_to.frame(frame_disqus)\n",
    "                        reactions = self.driver.find_elements(By.XPATH, \"//div[contains(@class,'reaction-item__enabled')]\")\n",
    "                        reactions[0].click()\n",
    "                        \n",
    "                        self.wait(timer_load_reactions)\n",
    "                        # refresh in case no voted\n",
    "                        reactions = self.driver.find_elements(By.XPATH, \"//div[contains(@class,'reaction-item__enabled')]\")\n",
    "                        \n",
    "\n",
    "                        def get_reactions_number(web_element_reaction):\n",
    "                            s = web_element_reaction.find_element(By.XPATH, \".//div[@class='reaction-item__votes']\").text\n",
    "                            n = 0\n",
    "                            if len(s) > 0:\n",
    "                                n = int(s.strip())\n",
    "                            \n",
    "                            return n\n",
    "\n",
    "                        reactions_like = get_reactions_number(reactions[0]) - 1\n",
    "                        reactions_funny = get_reactions_number(reactions[1])\n",
    "                        reactions_love = get_reactions_number(reactions[2])\n",
    "                        reactions_surprise = get_reactions_number(reactions[3])\n",
    "                        reactions_angry = get_reactions_number(reactions[4])\n",
    "                        reactions_sad = get_reactions_number(reactions[5])\n",
    "                except Exception as error2:\n",
    "                    print(\"Error with iframe parsing\", url, error2)\n",
    "                    self.cache[\"urls_error\"].append(url)\n",
    "                    self.cache[\"index_seen\"] += 1\n",
    "                    continue\n",
    "                \"\"\"\n",
    "                print(title, title_alternative1, title_alternative2)\n",
    "                print(rating,votes,type_serie)\n",
    "                print(cover, state, followers)\n",
    "                print(categories)\n",
    "                print(related, episodes)\n",
    "                \"\"\"\n",
    "\n",
    "                item_dict = {\n",
    "                    \"title\": title,\n",
    "                    \"title_alternative1\": title_alternative1,\n",
    "                    \"title_alternative2\": title_alternative2,\n",
    "                    \"rating\": rating,\n",
    "                    \"votes\": votes,\n",
    "                    \"type_serie\": type_serie,\n",
    "                    \"cover\": cover,\n",
    "                    \"state\": state,\n",
    "                    \"followers\": followers,\n",
    "                    \"categories\": categories,\n",
    "                    \"related\": related,\n",
    "                    \"episodes\": episodes,\n",
    "                    \"description\": description,\n",
    "                    \"reactions_like\": reactions_like,\n",
    "                    \"reactions_funny\": reactions_funny,\n",
    "                    \"reactions_love\": reactions_love,\n",
    "                    \"reactions_surprise\": reactions_surprise,\n",
    "                    \"reactions_angry\": reactions_angry,\n",
    "                    \"reactions_sad\": reactions_sad,\n",
    "                    \"reactions_total\": reactions_like+reactions_funny+reactions_love+reactions_surprise+reactions_angry+reactions_sad,\n",
    "                }\n",
    "                \n",
    "                item = ItemAnimeFLV()\n",
    "                item.set_attributes(item_dict)\n",
    "                # Warning: not parallelism/concurrency or RAM controled\n",
    "                df.at[index_df, :] = item.get_serie()\n",
    "                index_df += 1\n",
    "                self.cache[\"df\"] = df\n",
    "                self.cache[\"index_seen\"] += 1\n",
    "                \n",
    "                if self.stop:\n",
    "                    return df, False\n",
    "            print(\"Finished!\")\n",
    "            return df, True\n",
    "        except Exception as error:\n",
    "            print(\"General Error:\", error)\n",
    "            return df, False\n",
    "\n",
    "\n",
    "    def close(self):\n",
    "        print(\"CLOSING OBJECT...\")\n",
    "        if self.driver:\n",
    "            self.driver.close()\n",
    "            \n",
    "    def __del__(self):\n",
    "        print(\"DESTRUCTING OBJECT\")\n",
    "        if not self.restarting:\n",
    "            self.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logged in!\n"
     ]
    }
   ],
   "source": [
    "\n",
    "    \n",
    "global scrapper\n",
    "\n",
    "f = open(\".env.json\", \"r\")\n",
    "env = json.load(f)\n",
    "f.close()\n",
    "scrapper = ScrapperAnimeFLV(path_driver_chrome = \"/home/magody/chromedriver_linux64/chromedriver\")\n",
    "scrapper.driver.get(\"https://www3.animeflv.net/\")\n",
    "scrapper.wait(5)\n",
    "clear_output()\n",
    "scrapper.login(env[\"username_animeflv\"], env[\"password_animeflv\"])\n",
    "del env[\"password_animeflv\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Begin work, don't open or touch files in jobs/job2\n",
      "Pending work...8 urls in queue\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "04dfbb0541c54fd1ace16ad93ae7f3cf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=8.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stopping scraper... False <class 'NoneType'>\n",
      "Saving job...\n",
      "Closing with: 6 pending urls and 0 error urls\n",
      "Problems with sys.exit!\n",
      "\n",
      "General Error: HTTPConnectionPool(host='localhost', port=37909): Max retries exceeded with url: /session/2238fc345d9b4b49175e67f951da4672/elements (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7f686aacc0d0>: Failed to establish a new connection: [Errno 111] Connection refused'))\n",
      "scrape_items ended\n",
      "Stopping scraper... False <class 'pandas.core.frame.DataFrame'>\n",
      "Saving job...\n",
      "Closing with: 6 pending urls and 0 error urls\n"
     ]
    }
   ],
   "source": [
    "import signal\n",
    "import sys\n",
    "\n",
    "clear_output()\n",
    "\n",
    "global job, file_job\n",
    "\n",
    "job_name = \"job2\"\n",
    "folder_job = os.path.join(\"jobs\",job_name)\n",
    "file_job = os.path.join(folder_job,\"info.json\")\n",
    "\n",
    "if os.path.isdir(folder_job):\n",
    "    job = load_job(file_job)\n",
    "else:\n",
    "    os.mkdir(folder_job)\n",
    "    job = get_job_template(job_name)\n",
    "    save_job(file_job, job)\n",
    "    \n",
    "\n",
    "def close_job(is_completed, df):\n",
    "    global scrapper\n",
    "    global file_job, job \n",
    "    print(\"Stopping scraper...\", is_completed, type(df))\n",
    "    scrapper.stop = True\n",
    "    \n",
    "    print(\"Saving job...\")\n",
    "    \n",
    "    pending = []\n",
    "    if not is_completed:\n",
    "        index_seen = scrapper.cache.get(\"index_seen\",-1)\n",
    "        pending = scrapper.cache.get(\"urls\",[])\n",
    "        if len(pending) > 0:\n",
    "            pending = pending[index_seen+1:]\n",
    "          \n",
    "        df = scrapper.cache.get(\"df\", None)\n",
    "        \n",
    "    job['urls_queued'] = pending\n",
    "    \n",
    "    job['urls_error'] = scrapper.cache.get(\"urls_error\", [])\n",
    "    save_job(file_job, job)\n",
    "    \n",
    "    if df is not None:\n",
    "        df.to_csv(f\"temp/db_{job['name']}_items{job['execution_time']}.csv\", index=False)\n",
    "    \n",
    "    print(f\"Closing with: {len(job['urls_queued'])} pending urls and {len(job['urls_error'])} error urls\")\n",
    "    \n",
    "\n",
    "def signal_handler(sig, frame):\n",
    "    close_job(False, None)\n",
    "    \n",
    "    try:\n",
    "        sys.exit()\n",
    "    except:\n",
    "        print(\"Problems with sys.exit!\")\n",
    "        sys.exit()\n",
    "        \n",
    "signal.signal(signal.SIGINT, signal_handler)\n",
    "\n",
    "df = pd.DataFrame()\n",
    "\n",
    "print(f\"Begin work, don't open or touch files in {folder_job}\")\n",
    "\n",
    "timers = {\n",
    "    \"timer_load_page\": 1,\n",
    "    \"timer_load_disqus\": 1.5,\n",
    "    \"timer_load_reactions\": 1\n",
    "}\n",
    "\n",
    "state = job[\"state\"]\n",
    "if state == State.BEGIN:\n",
    "    print(\"Begining\")\n",
    "    df = scrapper.scrape_items(timers, url_begin=\"https://www3.animeflv.net/browse?page=1\")\n",
    "    close_job(True, df)\n",
    "elif state == State.PENDING:\n",
    "    scrapper.stop = False\n",
    "    urls_items = job[\"urls_queued\"]\n",
    "    urls_items.extend(job[\"urls_error\"])  # try again the errors\n",
    "    job[\"urls_error\"] = []\n",
    "    job[\"execution_time\"] += 1\n",
    "    print(f\"Pending work...{len(urls_items)} urls in queue\")\n",
    "    df, completed = scrapper.scrape_items(timers, urls=urls_items)\n",
    "    print(\"scrape_items ended\")\n",
    "    if completed: \n",
    "        job[\"state\"] = State.END\n",
    "    close_job(completed, df)\n",
    "elif state == State.END:\n",
    "    print(\"Work already completed!\")\n",
    "else:\n",
    "    print(\"Can't handle this state\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "a3c2402a762b1da2b664ca9cbb9344946d41b73132102685c4db1aa6c02b5b44"
  },
  "kernelspec": {
   "display_name": "Python 3.8.5 64-bit ('base': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
